# data wrangling
import pandas as pd
import numpy as np

# visualization
import seaborn as sns
import matplotlib

# other
import sklearn
import fairlearn
import math

# import functionality
import matplotlib.pyplot as plt
from fairlearn.metrics import MetricFrame, make_derived_metric
from sklearn.metrics import precision_score
from fairlearn.metrics import selection_rate, false_positive_rate, false_negative_rate
from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference

## Load data 
I used the raw data set. I still has a lot of nan values. I only remove nan entries for ethnicity

# description
description = pd.read_csv('data/WiDS_Datathon_2020_Dictionary.csv')
description_dict = description.set_index('Variable Name').to_dict(orient='index')
# data
df = pd.read_csv('data/training_v2.csv')

df.head()

new_df = df.dropna(subset=['ethnicity'])

### Check base rates

# by choosing y_pred to be ground truth instead of predictions, we can easily compute the base rate in the data
mf = MetricFrame(metrics={'base rate' : selection_rate}, 
                 y_true=new_df['hospital_death'], 
                 y_pred=new_df['hospital_death'], 
                 sensitive_features=new_df['ethnicity'])
display(mf.by_group)

# summarize demographic parity as the max difference between groups
print("base rate diff: %.2f" % mf.difference(method='between_groups'))

mean_selection = mf.by_group.mean()

num_tot = len(new_df)  # Gives number of rows
# print(num_tot)
num_dead = len(new_df.loc[(new_df['hospital_death'] == 1)])
selection_rate_tot = num_dead / num_tot
print(selection_rate_tot)

ethcounts = new_df['ethnicity'].value_counts() #ethcounts is used later
display(ethcounts)
ethcounts["Caucasian"]

display(mf.group_max())
display(mf.group_min())

num_rows = len(new_df) #used later
num_rows

### compute probability of dying or not in genreal P(Y=1) or P(Y=0)

PY1 = len(new_df.loc[(df['hospital_death'] == 1)])/num_rows
print(PY1)
PY0 = len(new_df.loc[(df['hospital_death'] == 0)])/num_rows
print(PY0)
print(PY1 + PY0)

### compute observed probabilities

# display(new_df['ethnicity'].value_counts())

ethniciies = new_df['ethnicity'].unique()  #get array of ethnicities

prob_obv_y1 = []
prob_obv_y0 = []

for ethnicity in ethniciies:
#     print(ethnicity)
#     print(new_df['ethnicity'].value_counts()[ethnicity])
#     num_eth_tot = new_df['ethnicity'].value_counts()[ethnicity]
    num_tot = num_rows
    num_eth_dead = len(new_df.loc[(new_df['ethnicity'] == ethnicity) & (new_df['hospital_death'] == 1)])
    num_eth_alive = len(new_df.loc[(new_df['ethnicity'] == ethnicity) & (new_df['hospital_death'] == 0)])
    obv_prop_Y1_eth = num_eth_dead/num_tot
    obv_prop_Y0_eth = num_eth_alive/num_tot
#     print(obv_prop_Y1_eth)
#     print(obv_prop_Y0_eth)
#     print(obv_prop_Y0_eth+ obv_prop_Y1_eth)
    prob_obv_y1.append(obv_prop_Y1_eth)
    prob_obv_y0.append(obv_prop_Y0_eth)
    
# print(prob_obv_y1)   
# print(prob_obv_y0)

probs = {
  "observed y=1": prob_obv_y1,
  "observed y=0": prob_obv_y0
}

df_probs = pd.DataFrame(probs, index = ethniciies)
display(df_probs)
df_probs["observed y=1"]['Caucasian']

### compute weights

Information = {'ethnicity': ['Caucasian', 'Caucasian', 'Hispanic', 'Hispanic', 'African American', 'African American', 'Asian', 'Asian',
       'Native American', 'Native American', 'Other/Unknown', 'Other/Unknown'],
               'class': [0, 1, 0, 1,0, 1,0, 1,0, 1,0, 1],                
               'weights': [0]*12}

weights_df = pd.DataFrame(Information)

# weights_df.loc[weights_df['ethnicity'] == "Caucasian" & weights_df['class'] ==0]

# weights_df.loc[(weights_df['ethnicity'] == "Caucasian") & (weights_df['class'] == 1)]['weights']

weights_df = weights_df.set_index(["ethnicity", "class"]) #, "class"

weights_df.loc['Caucasian',1]#[1]

weights_df#['Caucasian']

for ethnicity in ethniciies:
    for i in range(2):
        if i==1:
#             print(ethnicity)
#             print(i)
            P_exp = PY1 * (ethcounts[ethnicity]/num_rows)
            P_obs = df_probs["observed y=1"][ethnicity]
#             print(P_exp)
        else:
#             print(ethnicity)
#             print(i)
            P_exp = PY0 * (ethcounts[ethnicity]/num_rows)
            P_obs = df_probs["observed y=0"][ethnicity]
        weight = P_exp/P_obs
        weights_df.loc[ethnicity,i] = weight
        
display(weights_df)
# weights_df.loc['Caucasian', 0]['weights']

### Compute number to sample

Information_sampledf = {'ethnicity': ['Caucasian', 'Caucasian', 'Hispanic', 'Hispanic', 'African American', 'African American', 'Asian', 'Asian',
       'Native American', 'Native American', 'Other/Unknown', 'Other/Unknown'],
               'class': [0, 1, 0, 1,0, 1,0, 1,0, 1,0, 1],                
               'number to be sampled': [0]*12}

sample_numbers_df = pd.DataFrame(Information_sampledf)

sample_numbers_df = sample_numbers_df.set_index(["ethnicity", "class"]) 

for ethnicity in ethniciies:
    for i in range(2):
        if i==1:
            sample_num = round(weights_df.loc[ethnicity, 1]['weights'] \
                            * len(new_df.loc[(new_df['ethnicity'] == ethnicity) & (new_df['hospital_death'] == 1)]))
        else:
            sample_num = round(weights_df.loc[ethnicity, 0]['weights'] * \
                               len(new_df.loc[(new_df['ethnicity'] == ethnicity) & (new_df['hospital_death'] == 0)]))
        sample_numbers_df.loc[ethnicity,i] = sample_num
        
display(sample_numbers_df)
sample_numbers_df.loc['Caucasian', 0]['number to be sampled']

### Now the actual sampling

final_df = pd.DataFrame()

for ethnicity in ethniciies:
    for i in range(2):
        if i==1:
#             df['num_legs'].sample(n=3, random_state=1)
            sample_num = sample_numbers_df.loc[ethnicity, 1]['number to be sampled']
            #if we need to oversample
            if sample_num > len(new_df.loc[(new_df['ethnicity'] == ethnicity) & (new_df['hospital_death'] == 1)]):
                temp_df = new_df.loc[(new_df['ethnicity'] == ethnicity) & (new_df['hospital_death'] == 1)]\
                        .sample(n=sample_numbers_df.loc[ethnicity, 1]['number to be sampled'], replace=True, random_state=1)
            else: #undersample
                temp_df = new_df.loc[(new_df['ethnicity'] == ethnicity) & (new_df['hospital_death'] == 1)]\
                    .sample(n=sample_numbers_df.loc[ethnicity, 1]['number to be sampled'], random_state=1)
            final_df = pd.concat([final_df, temp_df])
        else:
            sample_num = sample_numbers_df.loc[ethnicity, 0]['number to be sampled']
            #if we need to oversample
            if sample_num > len(new_df.loc[(new_df['ethnicity'] == ethnicity) & (new_df['hospital_death'] == 0)]):
                temp_df = new_df.loc[(new_df['ethnicity'] == ethnicity) & (new_df['hospital_death'] == 0)]\
                        .sample(n=sample_numbers_df.loc[ethnicity, 0]['number to be sampled'], replace=True, random_state=1)
            else: #undersample
                temp_df = new_df.loc[(new_df['ethnicity'] == ethnicity) & (new_df['hospital_death'] == 0)]\
                    .sample(n=sample_numbers_df.loc[ethnicity, 0]['number to be sampled'], random_state=1)
            final_df = pd.concat([final_df, temp_df])
        
final_df.head()

### Check bases rates again

# by choosing y_pred to be ground truth instead of predictions, we can easily compute the base rate in the data
mf = MetricFrame(metrics={'base rate' : selection_rate}, 
                 y_true=final_df['hospital_death'], 
                 y_pred=final_df['hospital_death'], 
                 sensitive_features=final_df['ethnicity'])
display(mf.by_group)

# summarize demographic parity as the max difference between groups
print("base rate diff: %.2f" % mf.difference(method='between_groups'))

print(len(df))
print(len(new_df))
print(len(final_df))

